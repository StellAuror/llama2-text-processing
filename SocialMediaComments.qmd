---
title: "Untitled"
format: html
---

# Sentiment Analysis: Llama 3.1 (7B parameters) vs. Traditional Sentiment Model

## Applying LLMs Capabilities to Sentiment Evaluation
### Libraries
```{r}
pacman::p_load(
  "tidyverse",
  "caret"
)

source("main.R")
```


### Test dataset
```{r}
read_csv("data/test.csv") |>
  rename("Sentiment" = sentiment, "Text" = text) |>
  filter(!is.na(Sentiment)) -> dfSocial

dfSocial$Sentiment |> table()
```

### Llama3.1 (7B) Sentiment Evaluation (10 iterations)
```{r}
if(FALSE) {
  for (i in 1:10) {
  ### Process the response with llama3.1
  dfSocial$Text |>
    parallelSentiment() -> llamaSentiment
  
  
  ### Unify the text format
  llamaSentimentClean <- case_when(
    grepl(tolower(llamaSentiment), pattern = "positive") ~ "positive",
    grepl(tolower(llamaSentiment), pattern = "negative") ~ "negative",
    grepl(tolower(llamaSentiment), pattern = "other") ~ "neutral",
    T ~ "na"
  )
  #  llamaSentimentClean |> table()
  dfSocial$llama <- llamaSentimentClean
  
  
  ### Save cases/erorrs (not classified by llama)
  dir.create(glue("iteration{i}"))
  dfSocial[llamaSentimentClean == "na",] |>
    write_csv2(glue("iteration{i}/NotClassified.csv"))
  
  
  ### Save accuracy by sentiment
  dfSocial |>
    select(llama, Sentiment) |>
    mutate(n = 1) |>
    pivot_wider(names_from = "llama", values_from = "n") |>
    rowwise() |>
    mutate(across(where(is.list), .fns = ~length(.x))) |>
    pivot_longer(values_to = "n", names_to = "llama", cols = -1) |>
    group_by(Sentiment) |>
    summarize(
      count = sum(n),
      accuracy = round(if_else(llama == Sentiment, n, 0) / sum(n), 2)
    ) |>
    filter(accuracy != 0) |>
    write_csv2(glue("iteration{i}/Accuracy.csv"))
  
  ### Save confusion matrix
  dfSocial |>
    select(llama, Sentiment) |>
    mutate(n = 1) |>
    pivot_wider(names_from = "llama", values_from = "n") |>
    rowwise() |>
    mutate(across(where(is.list), .fns = ~length(.x))) |>
    select(Sentiment, neutral, positive, negative) |>
    ungroup() |>
    write_csv2(glue("iteration{i}/ConfusionMatrix.csv"))
  
  dfSocial |>
    select(textID, llama, Sentiment) |>
     write_csv2(glue("iteration{i}/IDs.csv"))
}
}
```

## Results Evalaution

### Iteration-wise comparison

```{r}
lapply(1:10, function(i) {
  read_csv2(glue("iteration{i}/Accuracy.csv"))
}) |>
  map_df(data.frame, .id = "iteration") -> dfAccuracy

dfAccuracy |>
  mutate(jitter = row_number()) |>
  group_by(Sentiment) |>
  mutate(accuracy = (accuracy - mean(accuracy)) * 100) |>
  ggplot(aes(
    color = Sentiment,
    y = accuracy,
    x = jitter,
    group = Sentiment
  )) +
  geom_segment(aes(
    xend = jitter,
    yend = 0,
  ), size = 15) +
  facet_wrap(~factor(iteration, levels = 1:10, ordered = T), scales = "free_x") +
  scale_x_continuous(expand = c(.2, .2)) +
  theme(
    axis.text.x = element_blank(),
    text = element_text(family = "Segoe UI", size = 16)
  ) +
  labs(
    x = "",
    y = "accuracy deviation (in percent points)",
    title = str_wrap("The fluctuations in the accuracy of data classification between iterations are small (not bigger than 1.1% point). No clear pattern is visible, it might indicate randomization as a key factor of fluctiations.")
  )
```

High variability of classification between model iterations indicates uncertainty of the result of a single sentence evaluation. Such a feature of LLM models may mean that sentences need to be evaluated multiple times in order to standardize the result based on multiple observations.

```{r}
lapply(1:10, function(i) {
  read_csv2(glue("iteration{i}/ConfusionMatrix.csv"))
}) |>
  map_df(data.frame, .id = "iteration") -> dfConfusion

dfConfusion %>%
  group_by(Sentiment) %>%
  summarise(
    sd_neutral = sd(neutral),
    sd_positive = sd(positive),
    sd_negative = sd(negative)
  ) |>
  pivot_longer(names_to = "Llama", values_to = "sd", cols = -1) |>
  mutate(sd = round(sd, 1)) |>
  ggplot() +
  geom_tile(aes(
    x = Sentiment,
    y = Llama,
    fill = sd
  )) +
  scale_fill_gradient2(low = "#fec286", mid = "#b83779", high = "#010210", midpoint = 6) +
  theme_minimal() +
  geom_text(aes(
    x = Sentiment,
    y = Llama,
    label = sd
  ), color = "white", size = 8, fontface = "bold") +
  labs(
    title = str_wrap("A large standard deviation in the case of a sentiment classified by LLama as neutral in the data set may indicate a certain type of model searching for a so-called second bottom. The smallest deviation between models occurs in the case of positive opinions classified as negative - it occured relatively hardly ever.")
  )

```

After averaging the classification results of both models and applying a cutoff function based on the hyperbolic tangent, it is observed that the results deviate significantly more from the outcomes of individual iterations. This indicates a high degree of variability (instability) in the model. Additionally, an extremely small number of cases were assigned to the neutral class.

```{r}
### Good case
#dfSocial |>
#  filter(textID == "0e8aa10a4e") |> pull("Text") |> cat()

lapply(1:10, function(i) {
  read_csv2(glue("iteration{i}/IDs.csv"))
}) |>
  map_df(data.frame, .id = "iteration") -> dfFlow

dfFlow |>
  pivot_wider(names_from = "iteration", values_from = "llama") |>
  mutate(across(3:12, ~case_when(
      .x == "positive" ~ 1,
      .x == "neutral" ~ 0,
      .x == "negative" ~ -1,
      TRUE ~ NA
    ))) |>
  rowwise() |>
  mutate(
    avgResult = round(mean(c_across(where(is.numeric)), na.rm = T), 2),
    avgSentiment = case_when(
      avgResult <= tanh(-1)~ "negative",
      avgResult >= tanh(1) ~ "positive",
      T ~ "neutral"
    )
  ) -> dfFlowS
  
dfFlowS |>
  mutate(
    avgCertainty = if_else(abs(avgResult) >= tanh(1), "class assgined", "neutral"),
    avgResult = if_else(abs(avgResult) == 1, as.character(avgResult), "At least partial uncertanity"),
    avgResult = case_when(avgResult == "-1" ~ "negative", avgResult == "1" ~ "positive", T ~ avgResult)
  ) |>
  group_by(avgResult, avgCertainty) |>
  summarise(n = n()) |>
  na.omit() |>
  ggplot(aes(x = n, y = as.factor(avgResult))) +
  geom_col(aes(fill = factor(avgCertainty, levels = c("neutral", "class assgined")))) +
  geom_text(aes(label = n), color = "white", size = 5.5, hjust = 1) +
  scale_fill_manual(values = c("#cccccc", "#5662f6")) +
  theme_minimal() +
  labs(
    x = "# of observations",
    y = "",
    fill = "Classification",
    title = "The vast majority of "
  )

```

```{r}
dfFlowS |> ggplot(aes(x = avgResult)) + 
  geom_density(size = 2, color = "#5662f6") +
  geom_vline(xintercept = tanh(-1), color = "#6a6a6a", linetype = "dotted", size = 1) +
  geom_vline(xintercept = tanh(1), color = "#6a6a6a", linetype = "dotted", size = 1) +
  theme_minimal()
```

###### TEXT PLACE HOLDER ##########
```{r}
dfFlowS |>
  rowwise() %>%
  mutate(
    resultVariance = if_else(
      any(c_across(`1`:`10`) == 1) && any(c_across(`1`:`10`) == -1),
      "extreme",
      "centric"
    ),
    sentimentVariance = paste0(avgSentiment, " - ", resultVariance)
  ) |> 
  group_by(sentimentVariance, avgSentiment, resultVariance) |>
  summarise(n = n()) |>
  na.omit() |>
  ggplot() +
  geom_col(aes(
    x = sentimentVariance,
    y = n,
    fill = resultVariance
  )) + 
  facet_wrap(~avgSentiment, scales = "free") +
  scale_fill_manual(values = c("#272635", "#B1E5F2")) +
  theme_minimal() +
  theme(strip.background = element_rect(fill = "#eeeeee", color = NA)) +
  labs(
    x = "", 
    y = "# of cases",
    fill = "Varianace between iterations"
  ) 
  
```

### Quantitive Comparison

The overall accuracy of the models is 57.67%, which suggests a moderate level of agreement between the two models. However, the Kappa statistic is 0.3764, indicating a fair level of agreement, but far from perfect. The Kappa score suggests that while there is some overlap in how both models classify sentiment, there is also a significant amount of disagreement.
The McNemar's test P-Value is less than 2.2e-16, which strongly indicates that the discrepancies between the models are statistically significant.

The analysis reveals that the two models show a reasonable level of agreement in classifying negative and positive sentiments, with the highest concordance for negative sentiments. However, there is substantial divergence when it comes to neutral sentiments, suggesting that this category is more challenging for the models to consistently agree upon. The data suggests that while the models can be somewhat reliable in distinguishing clear positive or negative sentiments, they are less effective with neutral sentiments, which could be a focus for further model refinement.

```{r}
dfFlowS |>
  group_by(Sentiment, avgSentiment) |>
  summarise(n = n()) |>
  pivot_wider(names_from = "avgSentiment", values_from = "n") |>
  column_to_rownames(var = "Sentiment") |>
  as.matrix() |> confusionMatrix()
```

Irony often expresses a negative sentiment using seemingly positive words, or vice versa. Analysis has shown that neutral sentiment has the lowest effectiveness in models (with low precision and recall). Irony can be classified as neutral when models fail to recognize its true tone, indicating that irony in such contexts is more challenging to detect. Llama 3.1, trained on large datasets that may include contexts where irony occurs, has the potential to better recognize subtle signals of irony, such as tone, sentence structure, or the use of specific words. However... Overinterpretation: Despite the potential benefits, LLM models may be prone to overinterpreting data. This could lead to the erroneous attribution of irony where none exists, as these models try to find subtle meanings or intentions, even if the text is straightforward and literal. With LLM models, there is a risk that they may attribute irony where it is absent, especially when they attempt to overinterpret simple statements as complex ones. This phenomenon may stem from the fact that LLMs are highly sensitive to context and may identify nuances that aren't actually present.


## Qualitive Comparison

```{r}
dfFlowS |>
    filter(Sentiment != avgSentiment) |>
  inner_join(dfSocial, join_by(textID == textID)) |>
  filter(
    textID %in% (
      dfFlowS |>
      filter(Sentiment != avgSentiment) |>
        pull("textID")
    )
  ) |>
  select(Text, "StdModel" = Sentiment.x, avgResult) 
```

Here, the text should be analyzed, preferably using several methods: absolute classification into 3 cases, and adding a 4th category indicating those with more context.







